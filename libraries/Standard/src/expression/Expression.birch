/**
 * Abstract interface for evaluating and differentiating expressions.
 *
 * @param Value Result type.
 *
 * @param x Evaluated value of the expression (optional).
 * @param flagConstant Is this a constant expression?
 *
 * ```mermaid
 * classDiagram
 *    Expression <|-- Random
 *    Expression <|-- BoxedValue
 *    Expression <|-- BoxedForm
 *
 *    link Expression "../Expression/"
 *    link Random "../Random/"
 *    link BoxedValue "../BoxedValue/"
 *    link BoxedForm "../BoxedForm/"
 *
 *    class Random {
 *      random argument
 *    }
 *    class BoxedValue {
 *      constant value
 *    }
 *    class BoxedForm {
 *      expression
 *    }
 * ```
 *
 * Delayed expressions (alternatively: lazy expressions, compute graphs,
 * expression templates) encode mathematical expressions that can be
 * evaluated, differentiated, and moved (using Markov kernels). They are
 * assembled using mathematical operators and functions much like ordinary
 * expressions, but where one or more operands or arguments are
 * [Random](../Random/) objects. Where an ordinary expression is evaluated
 * immediately into a result, delayed expressions evaluate to further
 * `Expression` objects.
 *
 * Simple delayed expressions are trees of subexpressions with `Random` or
 * [Boxed](../Boxed) objects at the leaves. In general, however, a delayed
 * expression can be a directed acyclic graph, as subexpressions may be reused
 * during assembly.
 *
 * ### Simple use
 *
 * @note
 *     Call `value()` on an `Expression` to evaluate it.
 *
 * The simplest use case of a delayed expression is to assemble it and then
 * evaluate it by calling `value()`.
 *
 * Once `value()` is called on an `Expression`, it and all subexpressions that
 * constitute it are rendered *constant*. This particularly affects any
 * `Random` objects in the expression, the value of which can no longer be
 * altered.
 *
 * Evaluations are memoized at *checkpoints*. Each `Expression` object that
 * occurs in the delayed expression forms such a checkpoint. Further calls to
 * `value()` are optimized to retrieve these memoized evaluations rather than
 * re-evaluated the whole expression.
 *
 * ### Advanced use
 * 
 * More elaborate use cases include computing gradients and applying Markov
 * kernels. Call `eval()` to evaluate the expression in the same way as for
 * `value()`, but without rendering it constant. Any `Random` objects in the
 * expression that have not been rendered constant by a previous call to
 * `value()` are considered *arguments* of the expression.
 *
 * Use `grad()` to compute the gradient of an expression with respect to its
 * arguments. The gradients are accumulated in the `Random` arguments and can
 * be retrieved from them.
 *
 * Use `set()` to update the value of `Random` arguments, then `move()` on
 * the whole expression to re-evaluate it.
 *
 * Use `value()`, not `eval()`, unless you are taking responsibility for
 * correctness (e.g. moving arguments in a manner invariant to some target
 * distribution, using a Markov kernel). Otherwise, program behavior may lack
 * self-consistency. Consider, for example:
 *
 *     if x.value() >= 0.0 {
 *       doThis();
 *     } else {
 *       doThat();
 *     }
 *
 * This is correct usage. Using `x.eval()` instead of `x.value()` here would
 * allow the value of `x` to be later changed to a negative value, and the
 * program would lack self-consistency, as it executed `doThis()` instead of
 * `doThat()` based on the previous value.
 *
 * ### Checkpoints and memoization
 *
 * `eval()` memoizes only at checkpoints defined by `Expression` objects. This
 * reduces memory use. Internally, `grad()` benefits from re-evaluating
 * expressions between checkpoints to memoize intermediate results. It does so
 * by calling `peek()`. These memoized intermediate results are cleared again
 * as `grad()` progresses.
 */
abstract class Expression<Value>(x:Value!?, flagConstant:Boolean) {
  /**
   * Memoized result.
   */
  x:Value!? <- x;

  /**
   * Accumulated upstream gradient.
   */
  phantom g;
  hpp{{
  std::optional<numbirch::Future<numbirch::Array<Real,
      numbirch::Future<Value>::ndims>>> g;
  }}

  /**
   * Count of number of parents, set by trace().
   */
  linkCount:Integer <- 1;

  /**
   * Counter used to obtain pre- and post-order traversals of the expression
   * graph.
   */
  visitCount:Integer <- 0;

  /**
   * Is this a constant expression?
   */
  flagConstant:Boolean <- flagConstant;
  
  /**
   * Is this a constant expression?
   */
  final function isConstant() -> Boolean {
    return flagConstant;
  }

  /**
   * Does this have a value?
   */
  final function hasValue() -> Boolean {
    return x?;
  }

  /**
   * Does this have a gradient?
   */
  final function hasGradient() -> Boolean {
    return g?;
  }

  /**
   * Number of rows in result.
   */
  final function rows() -> Integer {
    eval();
    return global.rows(x!);
  }
  
  /**
   * Number of columns in result.
   */
  final function columns() -> Integer {
    eval();
    return global.columns(x!);
  }

  /**
   * Length of result. This is synonymous with `rows()`.
   */
  final function length() -> Integer {
    return rows();
  }

  /**
   * Size of result. This is equal to `rows()*columns()`.
   */
  final function size() -> Integer {
    return rows()*columns();
  }

  /**
   * Evaluate and render constant.
   *
   * @return The result.
   */
  final function value() -> Value! {
    /* need to call eval() before constant() for Random in particular; a
     * Random may have an associated distribution from which it is yet to
     * simulate, constant() will remove this */
    eval();
    constant();
    return x!;
  }

  /**
   * Evaluate.
   *
   * @return The result.
   */
  final function eval() -> Value! {
    if !x? {
      doEval();
      assert x?;
    }
    return x!;
  }
  function doEval() {
    //
  }

  /**
   * Re-evaluate, ignoring memos. Memoizes at coarse-grain (i.e. Expression
   * objects, not forms).
   *
   * @param x Vectorized arguments.
   *
   * @return The result.
   */
  final function move(x:Real[_]) -> Value! {
    trace();
    visitor:MoveVisitor(x);
    let y <- move(visitor);
    assert visitor.isFinished();
    return y;
  }
  final function move(visitor:MoveVisitor) -> Value! {
    if !flagConstant {
      visitCount <- visitCount + 1;
      if visitCount == 1 {
        doMove(visitor);
        assert x?;
      }
      if visitCount >= linkCount {
        assert visitCount == linkCount;
        visitCount <- 0;  // reset for next time
      }
    }
    return x!;
  }
  function doMove(visitor:MoveVisitor) {
    //
  }

  /**
   * Vectorize arguments and gradients.
   *
   * @return The vectorized arguments and gradients.
   */
  final function args() -> (Real[_], Real[_]) {
    trace();
    visitor:ArgsVisitor;
    args(visitor);
    return visitor.args();
  }
  final function args(visitor:ArgsVisitor) {
    if !flagConstant {
      visitCount <- visitCount + 1;
      if visitCount == 1 {
        doArgs(visitor);
      }
      if visitCount >= linkCount {
        assert visitCount == linkCount;
        visitCount <- 0;  // reset for next time
      }
    }
  }
  function doArgs(visitor:ArgsVisitor) {
    //
  }

  /**
   * Evaluate gradient with respect to arguments. Clears memos at fine-grain.
   *
   * @param g Upstream gradient.
   *
   * The expression is treated as a function, and the arguments defined
   * as those `Random` objects in the expression that are not constant.
   *
   * If the expression encodes
   *
   * $$x_n = f(x_0) = (f_n \circ \cdots \circ f_1)(x_0),$$
   *
   * and this particular object encodes one of those functions
   * $x_i = f_i(x_{i-1})$, the upstream gradient `d` is
   *
   * $$\frac{\partial (f_n \circ \cdots \circ f_{i+1})}
   * {\partial x_i}\left(x_i\right).$$
   *
   * `grad()` then computes:
   *
   * $$\frac{\partial (f_n \circ \cdots \circ f_{i})}
   * {\partial x_{i-1}}\left(x_{i-1}\right),$$
   *
   * and passes the result to the next step in the chain, which encodes
   * $f_{i-1}$. The argument that encodes $x_0$ keeps the final result---it
   * is a `Random` object.
   *
   * Reverse-mode automatic differentiation is used. The previous call to
   * `eval()` constitutes the forward pass, and the call to `grad()` the
   * backward pass.
   *
   * Because expressions are, in general, directed acyclic graphs, a counting
   * mechanism is used to accumulate upstream gradients into any shared
   * subexpressions before visiting them. This ensures that each subexpression
   * is visited only once, not as many times as it is used. Mathematically,
   * this is equivalent to factorizing out the subexpression as a common
   * factor in the application of the chain rule. It turns out to be
   * particularly important when expressions include posterior parameters
   * after multiple Bayesian updates applied by automatic conditioning. Such
   * expressions can have many common subexpressions, and the counting
   * mechanism results in automatic differentiation of complexity $O(N)$ in
   * the number of updates, as opposed to $O(N^2)$ otherwise.
   */
  final function grad<Gradient>(g:Gradient) {
    trace();
    shallowGrad(g);
    deepGrad();
  }
  final function shallowGrad<Gradient>(g:Gradient) {
    if !flagConstant {
      visitCount <- visitCount + 1;
      if visitCount == 1 {
        /* (re-)start accumulation */
        this.g <- g;
      } else {
        /* continue accumulation */
        this.g <- this.g! + g;
      }
    }
  }
  function doShallowGrad() {
    //
  }
  final function deepGrad() {
    if !flagConstant && visitCount >= linkCount {
      assert visitCount == linkCount;
      visitCount <- 0;  // reset for next time
      doShallowGrad();
      doDeepGrad();
    }
  }
  function doDeepGrad() {
    //
  }

  /**
   * Evaluate.
   *
   * @return The result.
   */
  final function peek() -> Value! {
    /* forms use peek() for fine-grain memoization, but this terminates at
     * the nearest Expression objects; so here should behave as for eval(),
     * reverting to coarse-grain memoization */
    return eval();
  }

  /**
   * Trace an expression before calling grad() or move(). This traces through
   * the expression and, for each Expression object, updates the count of its
   * number of parents. This is necessary to ensure correct and efficient
   * execution of grad() and move(), as these counts ensure that each node in
   * the graph is visited exactly once.
   */
  final function trace() {
    reset();
    relink();
  }
  final function reset() {
    assert visitCount == 0;
    if !flagConstant && linkCount != 0 {
      linkCount <- 0;
      doReset();
    }
  }
  function doReset() {
    //
  }
  final function relink() {
    assert visitCount == 0;
    if !flagConstant {
      linkCount <- linkCount + 1;
      if linkCount == 1 {
        doRelink();
      }
    }
  }
  function doRelink() {
    //
  }

  /**
   * Render the entire expression constant.
   */
  final function constant() {
    if !flagConstant {
      g <- nil;
      linkCount <- 1;
      visitCount <- 0;
      flagConstant <- true;
      doConstant();
    }
  }
  function doConstant() {
    //
  }

  override function write(buffer:Buffer) {
    buffer.set(value()!);
  }
}
