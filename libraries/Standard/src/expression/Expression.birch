/**
 * Delayed expression.
 *
 * - Value: Result type.
 *
 * Delayed expressions (alternatively: lazy expressions, compute graphs,
 * expression templates) encode mathematical expressions that can be
 * evaluated, differentiated, and moved (using Markov kernels). They are
 * assembled using mathematical operators and functions much like ordinary
 * expressions, but where one or more operands or arguments are
 * [Random](../Random/) objects. Where an ordinary expression is evaluated
 * immediately into a result, delayed expressions evaluate to further
 * `Expression` objects.
 *
 * Simple delayed expressions are trees of subexpressions with `Random` or
 * [Boxed](../Boxed) objects at the leaves. In general, however, a delayed
 * expression can be a directed acyclic graph, as subexpressions may be reused
 * during assembly.
 *
 * ### Simple use
 *
 * !!! tip
 *     Call `value()` on an `Expression` to evaluate it.

 * The simplest use case of a delayed expression is to assemble it and then
 * evaluate it by calling `value()`. Evaluations are memoized, so further
 * calls `value()` do not require re-evaluation, they simply return the
 * memoized value.
 *
 * Once `value()` is called on an `Expression`, it and all subexpressions that
 * constitute it are considered *constant*. This particularly affects any
 * `Random` objects in the expression, the value of which can no longer be
 * alterated.
 *
 * ### Advanced use
 *
 * !!! tip
 *     Consider calling `helper()` on an `Expression` to obtain an
 *     [ExpressionHelper](../ExpressionHelper/) wrapper around it---this
 *     provides a simpler interface if needing to differentiate or apply a
 *     Markov kernel.
 * 
 * More elaborate use cases include computing gradients and applying Markov
 * kernels. Call `eval()` to evaluate the expression in the same way as for
 * `value()`, but without rendering it constant. Any `Random` objects in the
 * expression that have not previously been rendered constant by a call to
 * `value()` are then considered *arguments* eligible for moving.
 *
 * After updating the value of arguments, use `reval()` to re-evaluate
 * the expression with those new values.
 *
 * At any time, use `grad()` to compute the gradient of an expression with
 * respect to its arguments. The gradient is accumulated into those arguments
 * (the `Random` objects).
 *
 * !!! tip
 *     `ExpressionHelper` makes it easy to update the value of arguments and
 *     collect gradients from them.
 *
 * Use `value()`, not `eval()`, unless you are taking responsibility for
 * correctness (e.g. moving arguments in a manner invariant to some
 * target distribution, using a Markov kernel). Otherwise, program
 * behavior may lack self-consistency. Consider, for example:
 *
 *     if x.value() >= 0.0 {
 *       doThis();
 *     } else {
 *       doThat();
 *     }
 *
 * This is correct usage. Using `eval()` instead of `value()` here allows
 * some other part of the code to later change the value of the random
 * variable `x` to a negative value, and the program lacks
 * self-consistency: it executed `doThis()` instead of `doThat()` based
 * on a previous value of `x`.
 *
 * !!! attention
 *     Correctness is the programmer's responsibility when using the advanced
 *     interface.
 */
abstract class Expression<Value> {
  /**
   * Memoized value.
   */
  x:Value?;

  /**
   * Generation in which the expression was evaluated. This is zero unless
   * `eval()` has been called with a given generation provided, and the
   * expression has not subsequently been made constant (in which case it
   * reverts to zero).
   */
  generation:Integer <- 0;

  /**
   * Number of times this appears as a subexpression during an evaluation.
   */
  evalCount:Integer16 <- 0;

  /**
   * Number of times this has been visited during computation of gradients,
   * collecting arguments, moving, etc. Used by visitors for pre- and post-
   * visit operations, such as collecting upstream gradients before
   * post-visit, etc.
   */
  visitCount:Integer16 <- 0;

  /**
   * Has `value()` been called? This is used as a short-circuit for shared
   * subexpressions.
   */
  flagConstant:Boolean <- false;   ///@todo Replace with virtual function

  /**
   * Value assignment.
   */
  operator <- x:Value {
    setValue(x);
  }

  /**
   * Length of result. This is synonymous with `rows()`.
   */
  final function length() -> Integer {
    return rows();
  }

  /**
   * Size of result. This is equal to `rows()*columns()`.
   */
  final function size() -> Integer {
    return rows()*columns();
  }

  /**
   * Number of rows in result.
   */
  final function rows() -> Integer {
    if x? {
      return global.rows(x!);
    } else {
      return doRows();
    }
  }
  abstract function doRows() -> Integer;
  
  /**
   * Number of columns in result.
   */
  final function columns() -> Integer {
    if x? {
      return global.columns(x!);
    } else {
      return doColumns();
    }
  }
  abstract function doColumns() -> Integer;

  /**
   * Depth of the expression tree.
   */
  final function depth() -> Integer {
    if isConstant() {
      return 1;
    } else {
      return doDepth();
    }
  }
  abstract function doDepth() -> Integer;

  /**
   * Is this a Random expression?
   */
  function isRandom() -> Boolean {
    return false;
  }
  
  /**
   * Is this a constant expression?
   */
  final function isConstant() -> Boolean {
    return flagConstant;
  }

  /**
   * Does this have a value?
   *
   * Returns: true if a value has been evaluated with `value()` or `eval()`,
   * false otherwise.
   *
   * Note that this differs from `isConstant()`, which will only return true
   * for a value that has been evaluated with `value()`.
   */
  final function hasValue() -> Boolean {
    return x?;
  }

  /**
   * Set the value.
   *
   * - x: Value.
   */
  final function setValue(x:Value) {
    assert !this.x?;
    this.x <- x;
    value();
  }

  /**
   * Set the evaluated value.
   *
   * - x: Evaluated value.
   */
  final function setEval(x:Value) {
    assert !this.x?;
    this.x <- x;
  }

  /**
   * Evaluate and make constant.
   *
   * Returns: The evaluated value of the expression.
   *
   * An expression is considered constant once `value()` has been called on
   * it. All subexpressions are necessarily also then constant, including any
   * `Random` objects that occur, which are no longer considered arguments for
   * the purpose of `grad()`.
   */
  final function value() -> Value {
    if !isConstant() {
      doValue();
      if !hasValue() {
        doCompute();
      }
      doConstant();
      doClearGrad();
      generation <- 0;
      evalCount <- 0;
      visitCount <- 0;
      flagConstant <- true;
    }
    return x!;
  }

  /**
   * Evaluate and keep variable.
   *
   * Returns: The evaluated value of the expression.
   */
  final function eval() -> Value {
    return eval(1);
  }

  /**
   * Evaluate and keep variable.
   *
   * - gen: Generation.
   *
   * Returns: The evaluated value of the expression.
   *
   * If the object has not previously be assigned a generation, it is assigned
   * to `gen`.
   */
  final function eval(gen:Integer) -> Value {
    if !isConstant() {
      if !hasValue() {
        doEval(gen);
        doCompute();
      }
      if generation == 0 {
        generation <- gen;
      }
    }
    return x!;
  }

  /*
   * Evaluate with count; used internally.
   */
  final function countEval(gen:Integer) -> Value {
    if !isConstant() {
      if !hasValue() {
        doEval(gen);
        doCompute();
      }
      if generation == 0 {
        generation <- gen;
      }
      evalCount <- evalCount + 1;
    }
    return x!;
  }

  /**
   * Re-evaluate and keep variable.
   *
   * Returns: The evaluated value of the expression.
   */
  final function reval() -> Value {
    return reval(1);
  }

  /**
   * Re-evaluate and keep variable.
   *
   * - gen: Generation limit.
   *
   * Returns: The evaluated value of the expression.
   *
   * The generation limit `gen` works to truncate the recursion, as for
   * `grad()`.
   */
  final function reval(gen:Integer) -> Value {
    assert hasValue();
    if !isConstant() && generation >= gen {
      assert evalCount == 0;
      doReval(gen);
      doCompute();
    }
    return x!;
  }

  /*
   * Re-evaluate with count; used internally.
   */
  final function countReval(gen:Integer) -> Value {
    assert hasValue();
    if !isConstant() && generation >= gen {
      if visitCount == 0 {
        doReval(gen);
        doCompute();
      }
      visitCount <- visitCount + 1;
      if visitCount == evalCount {
        visitCount <- 0;  // reset for next time
      }
    }
    return x!;
  }

  /**
   * Evaluate gradient of the expression with respect to its arguments.
   *
   * - d: Upstream gradient.
   */
  final function grad<Gradient>(d:Gradient) {
    return grad(1, d);
  }

  /**
   * Evaluate gradient of the expression with respect to its arguments.
   *
   * - gen: Generation limit.
   * - d: Upstream gradient.
   *
   * `eval()` must have been called before calling `grad()`.
   *
   * The expression is treated as a function, and the arguments defined
   * as those `Random` objects in the expression that are not constant.
   *
   * The generation limit `gen` is used to truncate the recursion. Any
   * expressions that have been assigned a generation number less than
   * `gen` (usually at the time they are evaluated with `eval()`), are
   * considered constant for the purposes of differentiation. A value of
   * less than or equal to zero will not truncate the recursion.
   *
   * If the expression encodes
   *
   * $$x_n = f(x_0) = (f_n \circ \cdots \circ f_1)(x_0),$$
   *
   * and this particular object encodes one of those functions
   * $x_i = f_i(x_{i-1})$, the upstream gradient `d` is
   *
   * $$\frac{\partial (f_n \circ \cdots \circ f_{i+1})}
   * {\partial x_i}\left(x_i\right).$$
   *
   * `grad()` then computes:
   *
   * $$\frac{\partial (f_n \circ \cdots \circ f_{i})}
   * {\partial x_{i-1}}\left(x_{i-1}\right),$$
   *
   * and passes the result to the next step in the chain, which encodes
   * $f_{i-1}$. The argument that encodes $x_0$ keeps the final result---it
   * is a `Random` object.
   *
   * Reverse-mode automatic differentiation is used. The previous call to
   * `eval()` constitutes the forward pass, and the call to `grad()` the
   * backward pass.
   *
   * Because expressions are, in general, directed acyclic graphs, a counting
   * mechanism is used to accumulate upstream gradients into any shared
   * subexpressions before visiting them. This ensures that each subexpression
   * is visited only once, not as many times as it is used. Mathematically,
   * this is equivalent to factorizing out the subexpression as a common
   * factor in the application of the chain rule. It turns out to be
   * particularly important when expressions include posterior parameters
   * after multiple Bayesian updates applied by automatic conditioning. Such
   * expressions can have many common subexpressions, and the counting
   * mechanism results in automatic differentiation of complexity $O(N)$ in
   * the number of updates, as opposed to $O(N^2)$ otherwise.
   */
  final function grad<Gradient>(gen:Integer, d:Gradient) {
    assert visitCount == 0;
    if generation < gen {
      value();  // make constant
    } else if !isConstant() {
      assert evalCount == 0;
      doClearGrad();  // start accumulation
      doAccumulateGrad(d);
      doGrad(gen);
      if !isRandom() {
        doClearGrad();  // clear intermediate gradients to save memory
      }
    }
  }

  /*
   * Evaluate gradient with count; used internally.
   */
  final function countGrad<Gradient>(gen:Integer, d:Gradient) {
    if generation < gen {
      value();  // make constant
    } else if !isConstant() {
      assert evalCount > 0;
      if visitCount == 0 {
        doClearGrad();  // start accumulation
      }
      doAccumulateGrad(d);
      visitCount <- visitCount + 1;
      if visitCount == evalCount {
        /* upstream gradients accumulated */
        doGrad(gen);
        if !isRandom() {
          doClearGrad();  // clear intermediate gradients to save memory
        }
        visitCount <- 0;  // reset for next time
      }
    }
  }

  /*
   * Evaluate gradient with count; used internally.
   */
  final function countGrad(gen:Integer, d:Real, i:Integer) {
    if generation < gen {
      value();  // make constant
    } else if !isConstant() {
      assert evalCount > 0;
      if visitCount == 0 {
        doClearGrad();  // start accumulation
      }
      doAccumulateGrad(d, i);
      visitCount <- visitCount + 1;
      if visitCount == evalCount {
        /* upstream gradients accumulated */
        doGrad(gen);
        if !isRandom() {
          doClearGrad();  // clear intermediate gradients to save memory
        }
        visitCount <- 0;  // reset for next time
      }
    }
  }

  /*
   * Evaluate gradient with count; used internally.
   */
  final function countGrad(gen:Integer, d:Real, i:Integer, j:Integer) {
    if generation < gen {
      value();  // make constant
    } else if !isConstant() {
      assert evalCount > 0;
      if visitCount == 0 {
        doClearGrad();  // start accumulation
      }
      doAccumulateGrad(d, i, j);
      visitCount <- visitCount + 1;
      if visitCount == evalCount {
        /* upstream gradients accumulated */
        doGrad(gen);
        if !isRandom() {
          doClearGrad();  // clear intermediate gradients to save memory
        }
        visitCount <- 0;  // reset for next time
      }
    }
  }

  /*
   * Evaluate arguments for value(); used internally.
   */
  abstract function doValue();

  /*
   * Evaluate arguments for eval() and countEval(); used internally.
   */
  abstract function doEval(gen:Integer);

  /*
   * Evaluate arguments for reval() and countReval(); used internally.
   */
  abstract function doReval(gen:Integer);

  /*
   * Make arguments constant; used internally.
   */
  abstract function doConstant();

  /*
   * Evaluate expression for value(), eval(), or reval(); used internally.
   */
  abstract function doCompute();

  /*
   * Accumulate gradient for grad(); used internally.
   */
  function doAccumulateGrad(d:Real) {
    assert false;
  }

  /*
   * Accumulate gradient for grad(); used internally.
   */
  function doAccumulateGrad(d:Real[_]) {
    assert false;
  }

  /*
   * Accumulate gradient for grad(); used internally.
   */
  function doAccumulateGrad(d:Real[_,_]) {
    assert false;
  }

  /*
   * Accumulate gradient for grad(); used internally.
   */
  function doAccumulateGrad(d:Real, i:Integer) {
    assert false;
  }

  /*
   * Accumulate gradient for grad(); used internally.
   */
  function doAccumulateGrad(d:Real, i:Integer, j:Integer) {
    assert false;
  }

  /*
   * Compute gradient for grad(); used internally.
   */
  abstract function doGrad(gen:Integer);

  /*
   * Clear accumulated gradient; used internally.
   */
  abstract function doClearGrad();

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftBeta() -> Beta? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftDirichlet() -> Dirichlet? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftRestaurant() -> Restaurant? {
    return nil;
  }
  
  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftGamma() -> Gamma? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftScaledGamma() ->  TransformLinear<Gamma>? {
    return nil;
  }
  
  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftInverseGamma() -> InverseGamma? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftInverseWishart() -> InverseWishart? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftGaussian() -> Gaussian? {
    return nil;
  }
  
  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftLinearGaussian() -> TransformLinear<Gaussian>? {
    return nil;
  }
  
  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftNormalInverseGamma(compare:Distribution<Real>) ->
      NormalInverseGamma? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftLinearNormalInverseGamma(compare:Distribution<Real>) ->
      TransformLinear<NormalInverseGamma>? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftMultivariateGaussian() -> MultivariateGaussian? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftLinearMultivariateGaussian() ->
      TransformLinearMultivariate<MultivariateGaussian>? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftDotMultivariateGaussian() ->
      TransformDot<MultivariateGaussian>? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftMultivariateNormalInverseGamma(compare:Distribution<Real>) ->
      MultivariateNormalInverseGamma? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftLinearMultivariateNormalInverseGamma(compare:Distribution<Real>) ->
      TransformLinearMultivariate<MultivariateNormalInverseGamma>? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftDotMultivariateNormalInverseGamma(compare:Distribution<Real>) ->
      TransformDot<MultivariateNormalInverseGamma>? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftMatrixGaussian() -> MatrixGaussian? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftLinearMatrixGaussian() ->
      TransformLinearMatrix<MatrixGaussian>? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftDotMatrixGaussian() ->
      TransformDotMultivariate<MatrixGaussian>? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftMatrixNormalInverseWishart(compare:Distribution<LLT>) ->
      MatrixNormalInverseWishart? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftLinearMatrixNormalInverseWishart(compare:Distribution<LLT>) ->
      TransformLinearMatrix<MatrixNormalInverseWishart>? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftDotMatrixNormalInverseWishart(compare:Distribution<LLT>) ->
      TransformDotMultivariate<MatrixNormalInverseWishart>? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftDiscrete() -> Discrete? {
    return nil;
  }

  /*
   * Attempt to graft this expression onto the delayed sampling graph.
   *
   * Return: The node if successful, nil if not.
   */
  function graftBoundedDiscrete() -> BoundedDiscrete? {
    return nil;
  }

  override function read(buffer:Buffer) {
    this <-? buffer.get<Value>();
  }

  override function write(buffer:Buffer) {
    buffer.set(value());
  }
}

/**
 * Length of an Expression.
 */
function length<Value>(x:Expression<Value>) -> Integer {
  return x.length();
}

/**
 * Size of an Expression.
 */
function size<Value>(x:Expression<Value>) -> Integer {
  return x.size();
}

/**
 * Number of rows of an Expression.
 */
function rows<Value>(x:Expression<Value>) -> Integer {
  return x.rows();
}

/**
 * Number of columns of an Expression.
 */
function columns<Value>(x:Expression<Value>) -> Integer {
  return x.columns();
}
