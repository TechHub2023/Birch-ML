/**
 * Abstract interface for evaluating and differentiating expressions.
 *
 * - Value: Result type.
 *
 * - x: Evaluated value of the expression (optional).
 * - flagConstant: Is this a constant expression?
 *
 * ```mermaid
 * classDiagram
 *    Expression <|-- Random
 *    Expression <|-- BoxedValue
 *    Expression <|-- BoxedForm
 *
 *    link Expression "../Expression/"
 *    link Random "../Random/"
 *    link BoxedValue "../BoxedValue/"
 *    link BoxedForm "../BoxedForm/"
 *
 *    class Random {
 *      random argument
 *    }
 *    class BoxedValue {
 *      constant value
 *    }
 *    class BoxedForm {
 *      expression
 *    }
 * ```
 *
 * Delayed expressions (alternatively: lazy expressions, compute graphs,
 * expression templates) encode mathematical expressions that can be
 * evaluated, differentiated, and moved (using Markov kernels). They are
 * assembled using mathematical operators and functions much like ordinary
 * expressions, but where one or more operands or arguments are
 * [Random](../Random/) objects. Where an ordinary expression is evaluated
 * immediately into a result, delayed expressions evaluate to further
 * `Expression` objects.
 *
 * Simple delayed expressions are trees of subexpressions with `Random` or
 * [Boxed](../Boxed) objects at the leaves. In general, however, a delayed
 * expression can be a directed acyclic graph, as subexpressions may be reused
 * during assembly.
 *
 * ### Simple use
 *
 * !!! tip
 *     Call `value()` on an `Expression` to evaluate it.
 *
 * The simplest use case of a delayed expression is to assemble it and then
 * evaluate it by calling `value()`.
 *
 * Once `value()` is called on an `Expression`, it and all subexpressions that
 * constitute it are rendered *constant*. This particularly affects any
 * `Random` objects in the expression, the value of which can no longer be
 * altered.
 *
 * Evaluations are memoized at *checkpoints*. Each `Expression` object that
 * occurs in the delayed expression forms such a checkpoint. Further calls to
 * `value()` are optimized to retrieve these memoized evaluations rather than
 * re-evaluated the whole expression.
 *
 * ### Advanced use
 * 
 * More elaborate use cases include computing gradients and applying Markov
 * kernels. Call `eval()` to evaluate the expression in the same way as for
 * `value()`, but without rendering it constant. Any `Random` objects in the
 * expression that have not been rendered constant by a previous call to
 * `value()` are considered *arguments* of the expression.
 *
 * Use `grad()` to compute the gradient of an expression with respect to its
 * arguments. The gradients are accumulated in the `Random` arguments and can
 * be retrieved from them.
 *
 * Use `set()` to update the value of `Random` arguments, then `move()` on
 * the whole expression to re-evaluate it.
 *
 * Use `value()`, not `eval()`, unless you are taking responsibility for
 * correctness (e.g. moving arguments in a manner invariant to some target
 * distribution, using a Markov kernel). Otherwise, program behavior may lack
 * self-consistency. Consider, for example:
 *
 *     if x.value() >= 0.0 {
 *       doThis();
 *     } else {
 *       doThat();
 *     }
 *
 * This is correct usage. Using `x.eval()` instead of `x.value()` here would
 * allow the value of `x` to be later changed to a negative value, and the
 * program would lack self-consistency, as it executed `doThis()` instead of
 * `doThat()` based on the previous value.
 *
 * ### Checkpoints and memoization
 *
 * `eval()` memoizes only at checkpoints defined by `Expression` objects. This
 * reduces memory use. Internally, `grad()` benefits from re-evaluating
 * expressions between checkpoints to memoize intermediate results. It does so
 * by calling `peek()`. These memoized intermediate results are cleared again
 * as `grad()` progresses.
 */
abstract class Expression<Value>(x:Value!?, flagConstant:Boolean) < Delay {
  /**
   * Memoized result.
   */
  x:Value!? <- x;

  /**
   * Accumulated upstream gradient.
   */
  phantom g;
  hpp{{
  std::optional<numbirch::Future<numbirch::Array<Real,
      numbirch::Future<Value>::ndims>>> g;
  }}

  /**
   * Generation label.
   */
  gen:Integer <- 0;

  /**
   * Maximum generation label with which constant(Integer) has been called.
   * This is used to truncate recursions.
   */
  genMax:Integer <- 0;

  /**
   * Number of times `link()` has been called.
   */
  linkCount:Integer <- 0;

  /**
   * Counter used to obtain pre- and post-order traversals of the expression
   * graph.
   */
  visitCount:Integer <- 0;

  /**
   * Is this a constant expression?
   */
  flagConstant:Boolean <- flagConstant;

  /**
   * Is this a Random expression?
   */
  function isRandom() -> Boolean {
    return false;
  }
  
  /**
   * Is this a constant expression?
   */
  final function isConstant() -> Boolean {
    return flagConstant;
  }

  /**
   * Does this have a value?
   */
  final function hasValue() -> Boolean {
    return x?;
  }

  /**
   * Does this have a gradient?
   */
  final function hasGradient() -> Boolean {
    return g?;
  }

  /**
   * Number of rows in result.
   */
  final function rows() -> Integer {
    eval();
    return global.rows(x!);
  }
  
  /**
   * Number of columns in result.
   */
  final function columns() -> Integer {
    eval();
    return global.columns(x!);
  }

  /**
   * Length of result. This is synonymous with `rows()`.
   */
  final function length() -> Integer {
    return rows();
  }

  /**
   * Size of result. This is equal to `rows()*columns()`.
   */
  final function size() -> Integer {
    return rows()*columns();
  }

  /**
   * Evaluate and render constant.
   *
   * Returns: The result.
   */
  final function value() -> Value! {
    /* need to call eval() before constant() for Random in particular; a
     * Random may have an associated distribution from which it is yet to
     * simulate, constant() will remove this */
    eval();
    constant();
    return x!;
  }

  /**
   * Evaluate.
   *
   * Returns: The result.
   */
  function eval() -> Value! {
    if !x? {
      doEval();
      assert x?;
    }
    return x!;
  }
  function doEval() {
    //
  }

  /**
   * Re-evaluate, ignoring memos. Memoizes at coarse-grain (i.e. Expression
   * objects, not forms).
   *
   * - x: Vectorized arguments.
   *
   * Returns: The result.
   */
  final function move(x:Real[_]) -> Value! {
    visitor:MoveVisitor(x);
    return move(visitor);
  }
  final function move(visitor:MoveVisitor) -> Value! {
    if !flagConstant {
      if visitCount == 0 {
        doMove(visitor);
        assert x?;
      }
      visitCount <- visitCount + 1;
      if visitCount >= linkCount {
        assert visitCount == linkCount || linkCount == 0;
        visitCount <- 0;  // reset for next time
      }
    }
    return x!;
  }
  function doMove(visitor:MoveVisitor) {
    //
  }

  /**
   * Vectorize arguments and gradients.
   *
   * Returns: The vectorized arguments and gradients.
   */
  final function args() -> (Real[_], Real[_]) {
    visitor:ArgsVisitor;
    args(visitor);
    return visitor.args();
  }
  final function args(visitor:ArgsVisitor) {
    if !flagConstant {
      if visitCount == 0 {
        doArgs(visitor);
      }
      visitCount <- visitCount + 1;
      if visitCount >= linkCount {
        assert visitCount == linkCount || linkCount == 0;
        visitCount <- 0;  // reset for next time
      }
    }
  }
  function doArgs(visitor:ArgsVisitor) {
    //
  }

  /**
   * Evaluate gradient with respect to arguments. Clears memos at fine-grain.
   *
   * - g: Upstream gradient.
   *
   * The expression is treated as a function, and the arguments defined
   * as those `Random` objects in the expression that are not constant.
   *
   * If the expression encodes
   *
   * $$x_n = f(x_0) = (f_n \circ \cdots \circ f_1)(x_0),$$
   *
   * and this particular object encodes one of those functions
   * $x_i = f_i(x_{i-1})$, the upstream gradient `d` is
   *
   * $$\frac{\partial (f_n \circ \cdots \circ f_{i+1})}
   * {\partial x_i}\left(x_i\right).$$
   *
   * `grad()` then computes:
   *
   * $$\frac{\partial (f_n \circ \cdots \circ f_{i})}
   * {\partial x_{i-1}}\left(x_{i-1}\right),$$
   *
   * and passes the result to the next step in the chain, which encodes
   * $f_{i-1}$. The argument that encodes $x_0$ keeps the final result---it
   * is a `Random` object.
   *
   * Reverse-mode automatic differentiation is used. The previous call to
   * `eval()` constitutes the forward pass, and the call to `grad()` the
   * backward pass.
   *
   * Because expressions are, in general, directed acyclic graphs, a counting
   * mechanism is used to accumulate upstream gradients into any shared
   * subexpressions before visiting them. This ensures that each subexpression
   * is visited only once, not as many times as it is used. Mathematically,
   * this is equivalent to factorizing out the subexpression as a common
   * factor in the application of the chain rule. It turns out to be
   * particularly important when expressions include posterior parameters
   * after multiple Bayesian updates applied by automatic conditioning. Such
   * expressions can have many common subexpressions, and the counting
   * mechanism results in automatic differentiation of complexity $O(N)$ in
   * the number of updates, as opposed to $O(N^2)$ otherwise.
   */
  final function grad<Gradient>(g:Gradient) {
    this.g <- g;
    doShallowGrad();
    doDeepGrad();
  }
  final function shallowGrad<Gradient>(g:Gradient) {
    if !flagConstant {
      if visitCount == 0 {
        /* (re-)start accumulation */
        this.g <- g;
      } else {
        /* continue accumulation */
        this.g <- this.g! + g;
      }
      visitCount <- visitCount + 1;
    }
  }
  function doShallowGrad() {
    //
  }
  final function deepGrad() {
    if !flagConstant && visitCount >= linkCount {
      assert visitCount == linkCount || linkCount == 0;
      visitCount <- 0;  // reset for next time
      doShallowGrad();
      doDeepGrad();
    }
  }
  function doDeepGrad() {
    //
  }

  /**
   * Evaluate.
   *
   * Returns: The result.
   */
  function peek() -> Value! {
    /* forms use peek() for fine-grain memoization, but this terminates at
     * the nearest Expression objects; so here should behave as for eval(),
     * reverting to coarse-grain memoization */
    return eval();
  }

  /**
   * Register this object as a dependency of another expression.
   */
  final function link() {
    assert visitCount == 0;
    linkCount <- linkCount + 1;
  }
  function doLink() {
    // internally, called when this is construct to recursively call link()
    // on dependencies
  }

  /**
   * Label generations.
   *
   * - gen: Generation.
   *
   * Recursively labels all subexpressions with the given generation,
   * terminating on subexpressions that are already labeled.
   */
  final function label(gen:Integer) {
    if !flagConstant && this.gen == 0 {
      this.gen <- gen;
      doLabel(gen);
    }
  }
  function doLabel(gen:Integer) {
    //
  }

  /**
   * Make older generations constant.
   *
   * - gen: Generation limit.
   *
   * Removes subexpressions that are labeled with a generation less than
   * `gen`.
   */
  final function constant(gen:Integer) {
    if !flagConstant && gen > genMax {
      genMax <- gen;
      if this.gen < gen {
        constant();
      } else {
        doConstant(gen);
      }
    }
  }
  function doConstant(gen:Integer) {
    //
  }

  /**
   * Render the entire expression constant.
   */
  final function constant() {
    if !flagConstant {
      g <- nil;
      gen <- 0;
      genMax <- 0;
      linkCount <- 0;
      visitCount <- 0;
      flagConstant <- true;
      doConstant();
    }
  }
  function doConstant() {
    //
  }

  override function write(buffer:Buffer) {
    buffer.set(value()!);
  }
}
